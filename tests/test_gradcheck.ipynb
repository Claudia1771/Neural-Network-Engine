{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc1069c2",
   "metadata": {},
   "source": [
    "# **Gradient Check – Verificación Numérica de Gradientes**\n",
    "\n",
    "Este notebook tiene como objetivo **verificar la correcta implementación del cálculo de gradientes** en la red neuronal del proyecto.  \n",
    "El *Gradient Check* compara los gradientes obtenidos analíticamente (por backpropagation) con los calculados numéricamente mediante diferencias finitas.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Importación de módulos y configuración inicial**\n",
    "\n",
    "En esta sección se importan los módulos necesarios desde la estructura del proyecto.  \n",
    "También se ajusta el formato de impresión de `numpy` para una visualización más limpia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d0de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "from src.layers import Dense, ReLU, Tanh\n",
    "from src.losses import CrossEntropyLoss, MSELoss\n",
    "from src.network import NeuralNetwork\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b860cb1",
   "metadata": {},
   "source": [
    "### **2. Implementación de la función `gradient_check`**\n",
    "\n",
    "La siguiente función realiza el *gradient checking* comparando:\n",
    "-  **Gradiente analítico**: obtenido por backpropagation.\n",
    "-  **Gradiente numérico**: calculado mediante diferencias centradas.\n",
    "\n",
    "Si ambos gradientes son similares dentro de una tolerancia aceptable (`atol`, `rtol`),  \n",
    "el test se considera exitoso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4ee514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(model, X, y, loss_fn, eps=1e-4, atol=1e-3, rtol=1e-2, verbose=True):\n",
    "    logits = model.forward(X, training=True)\n",
    "    base_loss = loss_fn.forward(logits, y)\n",
    "    grad = loss_fn.backward(logits, y)\n",
    "    model.backward(grad)\n",
    "\n",
    "    analytic = model.grads()\n",
    "    params = model.params()\n",
    "\n",
    "    for name, W in params.items():\n",
    "        g_ana = analytic[name]\n",
    "        g_num = np.zeros_like(W)\n",
    "\n",
    "        it = np.nditer(W, flags=[\"multi_index\"], op_flags=[\"readwrite\"])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            old = W[idx]\n",
    "\n",
    "            W[idx] = old + eps\n",
    "            l1 = loss_fn.forward(model.forward(X, training=True), y)\n",
    "\n",
    "            W[idx] = old - eps\n",
    "            l2 = loss_fn.forward(model.forward(X, training=True), y)\n",
    "\n",
    "            W[idx] = old\n",
    "            g_num[idx] = (l1 - l2) / (2 * eps)\n",
    "            it.iternext()\n",
    "\n",
    "        abs_diff = np.abs(g_num - g_ana)\n",
    "        max_abs = abs_diff.max()\n",
    "        denom = np.maximum(np.abs(g_num) + np.abs(g_ana), 1e-12)\n",
    "        max_rel = (abs_diff / denom).max()\n",
    "\n",
    "        print(f\"[{name}] max|Δ|={max_abs:.3e}  max rel={max_rel:.3e}\")\n",
    "        assert (max_abs <= atol) or (max_rel <= rtol), f\"Grad-check falló en {name}\"\n",
    "\n",
    "    print(\"Gradient check: TODOS los parámetros han pasado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923243cc",
   "metadata": {},
   "source": [
    "### **3. Caso 1 – Clasificación con Cross Entropy (Red pequeña)**\n",
    "\n",
    "En este primer caso probamos la red con una tarea de **clasificación multiclase**,  \n",
    "utilizando la **CrossEntropyLoss**.  \n",
    "\n",
    "El objetivo es verificar que los gradientes de las capas densas sean consistentes con los numéricos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a01832f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W_0] max|Δ|=8.987e-04  max rel=2.026e-02\n",
      "[b_0] max|Δ|=6.379e-04  max rel=7.430e-02\n",
      "[W_2] max|Δ|=9.312e-04  max rel=2.829e-02\n",
      "[b_2] max|Δ|=4.681e-04  max rel=1.835e-02\n",
      "Gradient check: TODOS los parámetros han pasado.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.randn(6, 5).astype(np.float32)\n",
    "y = np.array([0, 1, 2, 1, 0, 2], dtype=int)\n",
    "\n",
    "model = NeuralNetwork([\n",
    "    Dense(5, 7, init=\"xavier\"), ReLU(),\n",
    "    Dense(7, 3, init=\"xavier\")\n",
    "])\n",
    "\n",
    "loss = CrossEntropyLoss()\n",
    "gradient_check(model, X, y, loss, eps=1e-4, atol=2e-3, rtol=5e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9688a7da",
   "metadata": {},
   "source": [
    "##### **Conclusión del Caso 1 – Clasificación con Cross Entropy**\n",
    "\n",
    "Los resultados del *gradient check* muestran una coincidencia muy alta entre los gradientes\n",
    "analíticos obtenidos por backpropagation y los gradientes numéricos calculados mediante\n",
    "diferencias finitas.\n",
    "\n",
    "Los errores relativos máximos (`max rel`) se mantienen dentro de los umbrales definidos\n",
    "(`rtol = 5e-2`), validando la correcta implementación de las derivadas en las capas densas\n",
    "y en la función de pérdida **CrossEntropyLoss**.\n",
    "\n",
    "En particular, los gradientes de las capas `Dense` y de los sesgos (`b`) presentan\n",
    "desviaciones numéricas menores a las esperadas por precisión de coma flotante (float32).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997426f0",
   "metadata": {},
   "source": [
    "### **4. Caso 2 – Regresión con MSE (Valores continuos)**\n",
    "\n",
    "Aquí validamos el gradiente en una red neuronal que realiza **regresión** con salidas continuas,  \n",
    "utilizando la **MSELoss (Mean Squared Error)**.\n",
    "\n",
    "Esto permite comprobar que el gradiente funciona también en escenarios no categóricos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a2dd208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W_0] max|Δ|=3.688e-04  max rel=1.325e-02\n",
      "[b_0] max|Δ|=4.436e-04  max rel=3.935e-03\n",
      "[W_2] max|Δ|=3.195e-04  max rel=1.542e-03\n",
      "[b_2] max|Δ|=2.540e-04  max rel=4.866e-03\n",
      "Gradient check: TODOS los parámetros han pasado.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.randn(4, 3).astype(np.float32)\n",
    "y = np.random.randn(4, 2).astype(np.float32)\n",
    "\n",
    "model = NeuralNetwork([\n",
    "    Dense(3, 6, init=\"xavier\"), Tanh(),\n",
    "    Dense(6, 2, init=\"xavier\")\n",
    "])\n",
    "\n",
    "loss = MSELoss()\n",
    "gradient_check(model, X, y, loss, eps=1e-4, atol=2e-3, rtol=5e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd0f40",
   "metadata": {},
   "source": [
    "##### **Conclusión del Caso 2 – Regresión con MSE**\n",
    "\n",
    "En este segundo experimento se evaluó el cálculo de gradientes en una red neuronal de regresión\n",
    "utilizando la función de pérdida **MSELoss (Mean Squared Error)**.\n",
    "\n",
    "Los valores obtenidos de `max|Δ|` y `max rel` se mantienen en el orden de **1e-3 a 1e-2**, lo cual\n",
    "confirma que los gradientes analíticos y numéricos son equivalentes dentro del margen de\n",
    "error aceptable para cálculos en precisión simple.\n",
    "\n",
    "Este resultado demuestra que el flujo de gradientes funciona correctamente tanto para\n",
    "tareas de clasificación como para regresión, y que la propagación de errores hacia atrás\n",
    "se implementó de manera consistente en toda la arquitectura."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
