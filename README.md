# ğŸ§  Neural Network Engine

**Autora:** Claudia  
**Universidad de Las Palmas de Gran Canaria (ULPGC)**  
**Proyecto Final:** Neural Network Engine â€” ImplementaciÃ³n completa de un motor de redes neuronales desde cero  
**Lenguaje:** Python  
**Entorno de trabajo:** Visual Studio Code  
**Repositorio:** [GitHub - Neural Network Engine](https://github.com/Claudia1771/Neural-Network-Engine)

---

## ğŸ“˜ DescripciÃ³n general

Este proyecto consiste en la **implementaciÃ³n integral y modular de un motor de redes neuronales completamente desde cero**, utilizando exclusivamente **NumPy** y sin apoyo de frameworks de deep learning.  

El objetivo ha sido comprender y replicar todos los procesos internos de una red neuronal moderna: **propagaciÃ³n hacia adelante (forward pass)**, **retropropagaciÃ³n del error (backpropagation)**, **ajuste de parÃ¡metros mediante optimizadores adaptativos**, y **mecanismos de regularizaciÃ³n, parada temprana y programaciÃ³n de tasa de aprendizaje**.  

La arquitectura ha sido diseÃ±ada siguiendo principios de **modularidad, extensibilidad y claridad**, permitiendo entender y modificar cada parte del flujo de entrenamiento.  
El motor se valida mediante **dos experimentos principales** (Iris y MNIST), y se acompaÃ±a de **notebooks explicativos, pruebas unitarias, y resultados visuales**.

---

## ğŸ§© Estructura del proyecto

```
Neural-Network-Engine/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers.py
â”‚   â”œâ”€â”€ losses.py
â”‚   â”œâ”€â”€ network.py
â”‚   â”œâ”€â”€ optimizers.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ demo_iris.ipynb
â”‚   â””â”€â”€ experiment_mnist.ipynb
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ iris_loss.png
â”‚   â”œâ”€â”€ iris_acc.png
â”‚   â”œâ”€â”€ iris_confusion.png
â”‚   â”œâ”€â”€ mnist_loss.png
â”‚   â”œâ”€â”€ mnist_acc.png
â”‚   â””â”€â”€ mnist_confusion.png
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ unit_tests.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ iris/iris.csv
â”‚   â””â”€â”€ mnist/*.gz
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

Cada archivo cumple una funciÃ³n especÃ­fica, garantizando **una separaciÃ³n total entre lÃ³gica, datos, experimentos y visualizaciÃ³n**.

---

## âš™ï¸ DescripciÃ³n tÃ©cnica por mÃ³dulos

### ğŸ§± `layers.py` â€” Capas y activaciones

Define la estructura base de toda red neuronal. Incluye:

- **Clase `Layer`**: interfaz abstracta comÃºn a todas las capas.
- **`Dense`**: capa totalmente conectada que realiza la transformaciÃ³n lineal `Y = XW + b`.
  - InicializaciÃ³n configurable: *Xavier* o *He*.
  - Almacena gradientes (`dW`, `db`) para retropropagaciÃ³n.
- **Funciones de activaciÃ³n**:  
  - `ReLU`: mantiene activaciones positivas y atenÃºa gradientes negativos.  
  - `Sigmoid` y `Tanh`: funciones clÃ¡sicas usadas en modelos pequeÃ±os.  
  - `Softmax`: convierte logits en probabilidades normalizadas para clasificaciÃ³n multiclase.
- **`Dropout`**: desactiva neuronas aleatoriamente durante el entrenamiento para evitar sobreajuste.

Estas capas implementan tanto **`forward()`** como **`backward()`**, permitiendo el cÃ¡lculo manual del gradiente en toda la red.

---

### ğŸ§® `losses.py` â€” Funciones de pÃ©rdida

Mide la discrepancia entre la predicciÃ³n y la realidad.

- **`MSELoss`** (Error CuadrÃ¡tico Medio): Ãºtil para regresiÃ³n.  
- **`CrossEntropyLoss`**: estÃ¡ndar en clasificaciÃ³n multiclase. Calcula la entropÃ­a cruzada directamente a partir de los logits, evitando inestabilidades numÃ©ricas mediante `log-sum-exp`.

Cada funciÃ³n define:
- `forward(y_pred, y_true)`: cÃ¡lculo de pÃ©rdida.  
- `backward(y_pred, y_true)`: gradiente para retropropagaciÃ³n.

---

### ğŸ”§ `optimizers.py` â€” MÃ©todos de optimizaciÃ³n

Implementa los principales algoritmos para actualizar los pesos de la red:

- **SGD**: descenso de gradiente clÃ¡sico, con soporte para *momentum* y *weight decay*.
- **Adam**: optimizador adaptativo que ajusta tasas de aprendizaje individuales. Ideal para tareas no estacionarias.
- **RMSProp**: usa promedios mÃ³viles del cuadrado de los gradientes, eficiente en escenarios ruidosos.

Cada optimizador implementa el mÃ©todo `step(params, grads)` que actualiza los pesos de manera independiente para cada parÃ¡metro.

---

### ğŸ§  `network.py` â€” NÃºcleo y entrenador

#### Clase `NeuralNetwork`
Coordina todas las capas, calculando el flujo completo:
1. **Forward pass**: secuencialmente por todas las capas.
2. **Backward pass**: retropropaga los gradientes hacia las capas previas.
3. **Persistencia**: permite guardar y cargar pesos (`save`, `load`).

#### Clase `Trainer`
Gestiona el ciclo completo de entrenamiento:
- **Mini-batches**
- **EvaluaciÃ³n por Ã©pocas**
- **Early Stopping**
- **Weight Decay**
- **LR Scheduling** (*Step Decay* y *Cosine Annealing*)
- Registro detallado de mÃ©tricas (`train_loss`, `val_loss`, `train_acc`, `val_acc`).

Incluye salida visual y restauraciÃ³n automÃ¡tica del mejor modelo encontrado durante el entrenamiento.

---

### ğŸ§° `utils.py` â€” Utilidades y mÃ©tricas

Incluye funciones para:
- Carga y preprocesamiento de **Iris** y **MNIST**.  
- NormalizaciÃ³n (`normalize_01`, `standardize`).  
- GeneraciÃ³n de mini-lotes (`batch_iterator`).  
- DivisiÃ³n en conjuntos (`train_val_test_split`).  
- CÃ¡lculo de mÃ©tricas (`accuracy`, `confusion_matrix`).  
- VisualizaciÃ³n de curvas de entrenamiento y matrices de confusiÃ³n.  

Todo el sistema estÃ¡ preparado para reproducibilidad mediante `set_seed()`.

---

### ğŸ§ª `tests/unit_tests.py` â€” ValidaciÃ³n

Incluye pruebas de:
- **GradCheck**: verificaciÃ³n numÃ©rica de gradientes mediante diferencias finitas.
- **Entrenamiento miniatura**: una red XOR que confirma convergencia real (disminuciÃ³n progresiva de la pÃ©rdida).  

El resultado esperado muestra un gradiente coherente (`rel_error < 5e-2`) y una pÃ©rdida final mucho menor que la inicial.

---

## ğŸ”¬ Experimentos y resultados

### ğŸŒ¸ Experimento 1 â€” *Iris Dataset*

**Arquitectura del modelo:**
```python
NeuralNetwork([
    Dense(4, 16, init="he"), ReLU(),
    Dense(16, 16, init="he"), ReLU(),
    Dense(16, 3, init="xavier")
])
```
**ConfiguraciÃ³n:**
| ParÃ¡metro | Valor |
|------------|--------|
| Optimizador | Adam |
| Learning Rate | 0.01 |
| PÃ©rdida | CrossEntropyLoss |
| Batch Size | 16 |
| Early Stopping | Activado (patience=20) |
| Ã‰pocas | 200 |

**Resultados:**
| MÃ©trica | Valor |
|----------|--------|
| Train Accuracy | 97.14 % |
| Test Accuracy | 100 % |
| PÃ©rdida final | 0.017 |

**AnÃ¡lisis:**  
El modelo converge rÃ¡pidamente, alcanzando una clasificaciÃ³n perfecta en el conjunto de prueba. Las curvas de pÃ©rdida y precisiÃ³n muestran una mejora constante sin sobreajuste.  

**Visualizaciones:**  
<p align="center">
  <img src="results/iris_loss.png" width="45%"/>  
  <img src="results/iris_acc.png" width="45%"/>
</p>

<p align="center">
  <img src="results/iris_confusion.png" width="50%"/>
</p>

---

### ğŸ”¢ Experimento 2 â€” *MNIST Dataset*

**Arquitectura del modelo:**
```python
NeuralNetwork([
    Dense(784, 256, init="he"), ReLU(),
    Dense(256, 128, init="he"), ReLU(),
    Dropout(0.2),
    Dense(128, 10, init="xavier")
])
```

**ConfiguraciÃ³n:**
| ParÃ¡metro | Valor |
|------------|--------|
| Optimizador | Adam |
| Learning Rate | 0.001 |
| PÃ©rdida | CrossEntropyLoss |
| Batch Size | 64 |
| Dropout | 0.2 |
| Early Stopping | Activado (patience=10) |
| Scheduler | Cosine Annealing |

**Resultados:**
| MÃ©trica | Valor |
|----------|--------|
| Train Accuracy | 99.46 % |
| Test Accuracy | 97.70 % |
| Ã‰pocas ejecutadas | 16 |

**AnÃ¡lisis:**  
El modelo logra una precisiÃ³n excelente con una generalizaciÃ³n sÃ³lida. Las curvas de entrenamiento muestran estabilidad, y la matriz de confusiÃ³n revela confusiones mÃ­nimas entre dÃ­gitos de forma similar (4 y 9).  

**Visualizaciones:**  
<p align="center">
  <img src="results/mnist_loss.png" width="45%"/>  
  <img src="results/mnist_acc.png" width="45%"/>
</p>

<p align="center">
  <img src="results/mnist_confusion.png" width="60%"/>
</p>

---

### ğŸ“Š Comparativa entre modelos

| CaracterÃ­stica | Iris | MNIST |
|----------------|------|--------|
| Entradas | 4 | 784 |
| Clases | 3 | 10 |
| PrecisiÃ³n de prueba | **100%** | **97.7%** |
| RegularizaciÃ³n | No | Dropout(0.2) |
| Early Stopping | SÃ­ | SÃ­ |
| Scheduler | Step Decay | Cosine Annealing |
| Tiempo de entrenamiento | 4s | 3min aprox |

Ambos experimentos demuestran que el motor puede **adaptarse tanto a problemas simples como complejos**, manteniendo consistencia, precisiÃ³n y eficiencia.

---

## ğŸ§ª Funcionalidades destacadas

- ImplementaciÃ³n **manual y verificable** de *forward* y *backward propagation*.
- **OptimizaciÃ³n adaptativa (Adam, RMSProp, SGD)**.  
- **RegularizaciÃ³n avanzada:** Dropout y Weight Decay.  
- **ProgramaciÃ³n dinÃ¡mica de tasa de aprendizaje (LR schedulers)**.  
- **Early Stopping automÃ¡tico.**  
- **VisualizaciÃ³n automÃ¡tica de mÃ©tricas.**  
- **VerificaciÃ³n de gradientes (GradCheck).**  
- **Reproducibilidad total (set_seed).**

---

## ğŸš€ EjecuciÃ³n del proyecto

```bash
git clone https://github.com/Claudia1771/Neural-Network-Engine.git
cd Neural-Network-Engine
pip install -r requirements.txt
```

### EjecuciÃ³n de experimentos
```bash
jupyter notebook notebooks/demo_iris.ipynb
jupyter notebook notebooks/experiment_mnist.ipynb
```

### Pruebas unitarias
```bash
python tests/unit_tests.py
```

---

## ğŸ§© ConclusiÃ³n

El proyecto **Neural Network Engine** representa una implementaciÃ³n completa, optimizada y educativa de un motor de redes neuronales moderno.  
Reproduce con precisiÃ³n los fundamentos matemÃ¡ticos y computacionales de los frameworks reales, pero con un nivel de **transparencia, control y comprensiÃ³n total del proceso de aprendizaje**.  

Los resultados en Iris y MNIST confirman la **eficiencia, escalabilidad y robustez** del motor, consolidÃ¡ndolo como un trabajo sÃ³lido, bien estructurado y con aplicaciÃ³n prÃ¡ctica en entornos acadÃ©micos y de investigaciÃ³n.

